# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h9tKbb82bWDF1odebdrPEvfh69Z4XjqY
"""

import tensorflow as tf
# tf.compat.v1.enable_eager_execution()
from tensorflow import keras
from tensorflow.keras.layers import *
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import UpSampling2D
from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D
from tensorflow.keras.layers import concatenate
from tensorflow.keras.layers import Multiply
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Input, Add,Subtract, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,AveragePooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.utils import plot_model
from tensorflow.keras.initializers import glorot_uniform
K.set_image_data_format('channels_last')
K.set_learning_phase(1)
from tensorflow.keras.layers import Dropout
from keras.layers.advanced_activations import PReLU

class Traditional_UNET(tf.keras.layers.Layer):
    def __init__(self, name="Traditional_UNET",**kwargs):
        super().__init__(name=name,**kwargs)
        self.conv1 = Conv2D(64,kernel_size=(3,3),padding = 'same',activation='relu')
        self.conv2 = Conv2D(128,kernel_size = (3,3),padding = 'same',activation='relu')
        self.conv3 = Conv2D(256,kernel_size = (3,3),padding = 'same',activation='relu')
        self.conv4 = Conv2D(512,kernel_size = (3,3),padding = 'same',activation='relu')
        self.conv5 = Conv2D(1024,kernel_size = (3,3),padding = 'same',activation='relu')
        self.conv6 = Conv2D(256,kernel_size = (3,3),padding = 'same',activation='relu')
        self.conv7 = Conv2D(128,kernel_size = (3,3),padding = 'same',activation='relu')
        self.conv8 = Conv2D(64,kernel_size = (3,3),padding = 'same',activation='relu')
        self.conv9 = Conv2D(64,kernel_size = (3,3),padding = 'same',activation='relu')
        self.max1 = MaxPooling2D((2, 2), strides=(2, 2))
        self.max2 = MaxPooling2D((2, 2), strides=(2, 2))
        self.max3 = MaxPooling2D((2, 2), strides=(2, 2))
        self.max4 = MaxPooling2D((2, 2), strides=(2, 2))
        self.concat1 = Concatenate()
        self.concat2 = Concatenate()
        self.concat3 = Concatenate()
        self.concat4 = Concatenate()
        self.up1 = UpSampling2D()
        self.up2 = UpSampling2D()
        self.up3 = UpSampling2D()
        self.up4 = UpSampling2D()
    def get_config(self):
        cfg = super().get_config()
        return cfg

        
    def call(self, X):
        # implement the traditional unet operatiom
        conv1 = self.conv1(X)
        max1 = self.max1(conv1)
        conv2 = self.conv2(max1)
        max2 = self.max2(conv2)
        conv3 = self.conv3(max2)
        max3 = self.max3(conv3)
        conv4 = self.conv4(max3)
        max4 = self.max4(conv4)
        conv5 = self.conv5(max4)
        concat1 = self.concat1([conv5,max4])
        
        conv6 = self.conv6(concat1)
        
        up1 = self.up1(conv6)
        
        concat2 = self.concat2([up1,max3])
        #print(concat2.shape)
        conv7 = self.conv7(concat2)
        up2= self.up2(conv7)
        concat3 = self.concat3([up2,max2])
        conv8 = self.conv8(concat3)
        up3 = self.up3(conv8)
        concat4 = self.concat4([up3,max1])
        up4 = self.up4(concat4)
        conv9 = self.conv9(up4)
        return conv9

class dilated_UNET(tf.keras.layers.Layer):
    def __init__(self, name="dilated_UNET",**kwargs):
        super().__init__(name=name,**kwargs)
        self.conv1 = Conv2D(64,kernel_size=(3,3),padding = 'same',activation='relu',dilation_rate=(2,2))
        self.conv2 = Conv2D(64,kernel_size = (3,3),padding = 'same',activation='relu',dilation_rate=(2,2))
        self.conv3 = Conv2D(64,kernel_size = (3,3),padding = 'same',activation='relu',dilation_rate=(2,2))
        self.conv4 = Conv2D(64,kernel_size = (3,3),padding = 'same',activation='relu',dilation_rate=(2,2))
        self.conv5 = Conv2D(64,kernel_size = (3,3),padding = 'same',activation='relu',dilation_rate=(2,2))
        self.conv6 = Conv2D(64,kernel_size = (3,3),padding = 'same',activation='relu',dilation_rate=(2,2))
        self.conv7 = Conv2D(64,kernel_size = (3,3),padding = 'same',activation='relu',dilation_rate=(2,2))
        self.conv8 = Conv2D(64,kernel_size = (3,3),padding = 'same',activation='relu',dilation_rate=(2,2))
        self.conv9 = Conv2D(64,kernel_size = (3,3),padding = 'same',activation='relu',dilation_rate=(2,2))
        self.add1 = Add()
        self.add2 = Add()
        self.add3 = Add()
        self.add4 = Add()

    def get_config(self):
        cfg = super().get_config()
        return cfg

        
    def call(self, X):
        # implement the traditional unet operatiom
        conv1 = self.conv1(X)
        conv2 = self.conv2(conv1)
        conv3 = self.conv3(conv2)
        conv4 = self.conv4(conv3)
        conv5 = self.conv5(conv4)
        add1 = self.add1([conv5,conv4])
        conv6 = self.conv6(add1)
        add2 = self.add2([conv6,conv3])
        conv7 = self.conv7(add2)
        add3 = self.add3([conv7,conv2])
        conv8 = self.conv8(add3)
        add4 = self.add4([conv8,conv1])
        conv9 = self.conv9(add4)
        return conv9